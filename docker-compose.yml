version: "3.8"

x-airflow-common: &airflow-common
  image: open-dataplatform-airflow
  volumes:
    - ./airflow/config/airflow.cfg:/opt/airflow/airflow.cfg
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/spark:/scripts/spark
    - ./hadoop/conf:/opt/hadoop/conf
    - ./spark/config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
  environment:
    POSTGRES_HOST: airflow-db
    POSTGRES_PORT: 5432
    HDFS_HOST: hdfs://namenode:8020
  depends_on:
    - airflow-db

services:
  # Hadoop
  namenode:
    container_name: namenode
    hostname: namenode
    image: open-dataplatform-hadoop-hdfs
    environment:
      NODE_TYPE: namenode
      HDFS_NAMENODE_USER: root
      CLUSTER_NAME: hadoop-cluster
    volumes:
      - namenode:/hadoop/dfs/namenode
      - ./hadoop/conf:/etc/hadoop
    ports:
      - 50070:50070

  datanode:
    container_name: datanode
    hostname: datanode
    image: open-dataplatform-hadoop-hdfs
    environment:
      NODE_TYPE: datanode
      HDFS_DATANODE_USER: root
    volumes:
      - datanode:/hadoop/dfs/datanode
      - ./hadoop/conf:/etc/hadoop
    ports:
      - 50075:50075
    depends_on:
      - namenode

  # hive
  hive-postgres:
    image: postgres:9.4
    container_name: hive-postgres
    hostname: hive-postgres
    ports:
      - 5433:5432
    volumes:
      - ./tmp/postgres-hive:/var/lib/postgresql/data
    environment:
      POSTGRES_PASSWORD: hive
      POSTGRES_USER: hive
      POSTGRES_DB: metastore

  hive-metastore:
    hostname: hive-metastore
    image: open-dataplatform-hive
    container_name: hive-metastore
    depends_on:
      - hive-postgres
      - datanode
    volumes:
      - ./hadoop/conf:/etc/hadoop
      - ./hadoop/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
    command: "hive --service metastore"
    ports:
      - 9083:9083

  hive-server:
    hostname: hive-server
    image: open-dataplatform-hive
    container_name: hive-server
    depends_on:
      - hive-metastore
    volumes:
      - ./hadoop/conf:/etc/hadoop
      - ./hadoop/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
    command: "hiveserver2"
    ports:
      - 10002:10002

  # Spark
  spark-master:
    container_name: spark-master
    image: open-dataplatform-spark
    ports:
      - 4040:4040
      - 4041:4041
    command:
      - master
    environment:
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 4041
    volumes:
      - ./hadoop/conf:/opt/hadoop/conf
      - ./spark/config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./spark/applications:/scripts

  spark-worker:
    hostname: spark-worker
    container_name: spark-worker
    image: open-dataplatform-spark
    ports:
      - 4042:4040
    command:
      - worker
    depends_on:
      - spark-master
    environment:
      SPARK_WORKER_WEBUI_PORT: 4040
      SPARK_MASTER_URL: spark-master:7077
    volumes:
      - ./hadoop/conf:/opt/hadoop/conf
      - ./spark/config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

    # Airflow
  airflow-db:
    hostname: airflow-db
    container_name: airflow_db
    image: postgres
    restart: always
    environment:
      POSTGRES_PASSWORD: airflow
      POSTGRES_USER: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./tmp/postgres-airflow:/var/lib/postgresql/data
    ports:
      - 5432:5432

  airflow-webserver:
    hostname: airflow-webserver
    container_name: airflow_webserver
    <<: *airflow-common
    command: "webserver -p 9090"
    ports:
      - "8793:8793"
      - "9090:9090"

  airflow-scheduler:
    hostname: airflow-scheduler
    container_name: airflow_scheduler
    <<: *airflow-common
    command: "scheduler"

  # trino
  trino:
    image: trinodb/trino:353
    container_name: trino
    ports:
      - 8089:8080
    volumes:
      - ./trino/config/catalog:/etc/trino/catalog
      - ./trino/config/jvm.config:/etc/trino/jvm.config
      - ./trino/config/config.properties:/etc/trino/config.properties

  sqlpad:
    image: sqlpad/sqlpad:6.7.1
    container_name: sqlpad
    ports:
      - "3001:3000"
    environment:
      SQLPAD_CONNECTIONS__datalake__name: DataLake
      SQLPAD_CONNECTIONS__datalake__driver: trino
      SQLPAD_CONNECTIONS__datalake__host: trino
      SQLPAD_CONNECTIONS__datalake__username: sqlpad
      SQLPAD_CONNECTIONS__datalake__catalog: hive
      SQLPAD_CONNECTIONS__clickhouse__name: Clickhouse
      SQLPAD_CONNECTIONS__clickhouse__driver: clickhouse
      SQLPAD_CONNECTIONS__clickhouse__host: clickhouse
      SQLPAD_CONNECTIONS__clickhouse__username: sqlpad
      SQLPAD_CONNECTIONS__clickhouse__password: sqlpad
      SQLPAD_AUTH_DISABLED: "true"
      SQLPAD_AUTH_DISABLED_DEFAULT_ROLE: "admin"
    volumes:
      - sqlpad:/etc/sqlpad/seed-data

  clickhouse:
    container_name: clickhouse
    image: yandex/clickhouse-server:21.6.4.26
    volumes:
      - $PWD/clickhouse/config/users.xml:/etc/clickhouse-server/users.xml
      - clickhouse:/var/lib/clickhouse

volumes:
  namenode:
  datanode:
  sqlpad:
  clickhouse:

networks:
  default:
    name: open-dataplatform-network
